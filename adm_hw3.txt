#1. load the data.scv
    #read csv file with pd csv read with index_col = "Unnamed: 0"
    #2. calls cleandata() and return
        #gets just data which has not null in latitude column
        #dropna from description and title
        #drop_duplicates() remove all duplicated rows
#return data

#--------------------------------------------------------------------
#at this point we have a data frame with duplicated removed and empty fields removed.
#Our goal now is to go through each row of the data frame process it and write a tsv with it

#1. calls createAllReviews
    #Calls CreateTSV to all lines of the dataframe com .apply(lambda)
    #2.CreateTSV (inside a line of the dataframe the following will happen)
        #if description is empty (.isna) writes "NaN" else calls _nltkProcess
        #the same for title
            #3. _nltkProcess
                #string lower
                #4. calls _setupNltk
                    #Lazy initialization of objects needed to preprocess strings
                    #tokenizer = RegexpTokenizer(r'\w+')
                    #stopwords = set(stopwords.words('english'))
                    #stemmer = SnowballStemmer('english') 
                #tokenizer
                #remove stopwords
                #stemm
                #join the result and return
            #5. calls isEnglish() to test over each word int the line if they are in english and not chinese
                #try to encode into english, if fails return FALSE for test "is english"
                #if succeeds return TRUE for the test
            #if returned TRUE goes on to create the file, if False returns from CreateTSV and don't create the file
            #open the file to write using with open(self.dir_path+self.review_dir+"doc_"+str(x.name)+".tsv", 'w') as file:
            #try to write it
                #except (if fails to write) save the index (x.name) to delete the file afterwards
                
#----------------------------------------------------------
#At this point we have the data saved and preprocessed in each tsv file
#Our goal is to create a dictionary of words for all the documents together
# each word corresponding to a single number
#after this we can say in for each number (word) which documents have it
# example 
#{
#term_id_1:[document_1, document_2, document_4],
#term_id_2:[document_1, document_3, document_5, document_6],
#...}

#1BuildingEncoding
    #create an empty dictionary to store words
    #create a counter to store the index of how many single words we have (vocabulary size)
    #for each file in "listdir(directory address)"
        #goes to the title, strip its words and create a set of it to remove duplicates
        #do the same for the description
        #union both sets
        #for each word in the united set check if it is not dictionary already
            #if it isnt add it to the dictionary as a key
            #set the counter as the value of the word in the dictionary
            #increase the counter +1
#example of what we have at this point: {"car":1, "house":2}

#----------

#create a new dictionary with the keys being the numbers at the size of the vocabulary
    #this is our target dictionary, we want to say what documents have this word
#for each file in "listdir(directory address)"
    #goes to the title, strip its words and create a set of it to remove duplicates
    #do the same for the description
    #union both sets
    #for each word in the set
        #get which number this word has in the vocabulary dictionary
        #goes to the new dictionary and append the name of the document to the key

#NOW WE HAVE OUR TARGET DICTIONARY AND WE CAN DO THE INTERSECTION WITH THE QUERY

#-------------------------------------------------------
#our first go is to get the query and proccess it

#enter a string "a with garden house"
#get the string of the query and tokenize it
#filter it eliminating the stopwords
#stemm the filtered words and save it into a empty list
#the result is ['garden', 'hous']

#------------------------------------------------------
#now that we have the query words we can go to the dictionary 
#for every word in the query list
    #we append the set of documents that corresponds 
    #to the word number from the new dictionary into a new list
#this new list has all sets of documents which contain the query words
#as we want a document which has all the words in it we do an intersection of this documents

#WE THEN HAVE ALL THE DOCUMENTS WHICH HAS ALL THE WORDS FROM THE QUERY
