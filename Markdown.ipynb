{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T13:46:22.088990Z",
     "start_time": "2018-11-14T13:46:22.081011Z"
    }
   },
   "source": [
    "# Algorithmic Methods for Data Mining\n",
    "## Homework 3 - Airbnb Search Engine\n",
    "### Group #17 - Giulia Scialanga, Guilherme Nicchio, Marco Minici\n",
    "##### 26/11/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project consists in buiding a search engine over a data base of Airbnb houses. \n",
    "\n",
    "The code returns the houses of the data base which matches the descriptions entered by an user query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T11:55:05.188327Z",
     "start_time": "2018-11-16T11:55:05.182344Z"
    }
   },
   "source": [
    "User query means the sentence an user enter in a search field, for example: \"A beuatiful house with beach and garden\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project was used the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T12:04:34.996763Z",
     "start_time": "2018-11-16T12:04:09.181450Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import isdir\n",
    "from os import mkdir\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os import remove\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert quick description of the libraries used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized in a way that we have a csv file with the following columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T12:08:07.977213Z",
     "start_time": "2018-11-16T12:08:07.888328Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rate_per_night</th>\n",
       "      <th>bedrooms_count</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_listing</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$27</td>\n",
       "      <td>2</td>\n",
       "      <td>Humble</td>\n",
       "      <td>May 2016</td>\n",
       "      <td>Welcome to stay in private room with queen bed...</td>\n",
       "      <td>30.020138</td>\n",
       "      <td>-95.293996</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$149</td>\n",
       "      <td>4</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>November 2010</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW – ...</td>\n",
       "      <td>29.503068</td>\n",
       "      <td>-98.447688</td>\n",
       "      <td>Unique Location! Alamo Heights - Designer Insp...</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$59</td>\n",
       "      <td>1</td>\n",
       "      <td>Houston</td>\n",
       "      <td>January 2017</td>\n",
       "      <td>'River house on island close to the city' \\nA ...</td>\n",
       "      <td>29.829352</td>\n",
       "      <td>-95.081549</td>\n",
       "      <td>River house near the city</td>\n",
       "      <td>https://www.airbnb.com/rooms/16926307?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$60</td>\n",
       "      <td>1</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>February 2016</td>\n",
       "      <td>Private bedroom in a cute little home situated...</td>\n",
       "      <td>30.637304</td>\n",
       "      <td>-96.337846</td>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$75</td>\n",
       "      <td>2</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>February 2017</td>\n",
       "      <td>Welcome to our original 1920's home. We recent...</td>\n",
       "      <td>32.747097</td>\n",
       "      <td>-97.286434</td>\n",
       "      <td>The Porch</td>\n",
       "      <td>https://www.airbnb.com/rooms/17325114?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  average_rate_per_night bedrooms_count         city date_of_listing  \\\n",
       "1                    $27              2       Humble        May 2016   \n",
       "2                   $149              4  San Antonio   November 2010   \n",
       "3                    $59              1      Houston    January 2017   \n",
       "4                    $60              1        Bryan   February 2016   \n",
       "5                    $75              2   Fort Worth   February 2017   \n",
       "\n",
       "                                         description   latitude  longitude  \\\n",
       "1  Welcome to stay in private room with queen bed...  30.020138 -95.293996   \n",
       "2  Stylish, fully remodeled home in upscale NW – ...  29.503068 -98.447688   \n",
       "3  'River house on island close to the city' \\nA ...  29.829352 -95.081549   \n",
       "4  Private bedroom in a cute little home situated...  30.637304 -96.337846   \n",
       "5  Welcome to our original 1920's home. We recent...  32.747097 -97.286434   \n",
       "\n",
       "                                               title  \\\n",
       "1    2 Private rooms/bathroom 10min from IAH airport   \n",
       "2  Unique Location! Alamo Heights - Designer Insp...   \n",
       "3                          River house near the city   \n",
       "4                       Private Room Close to Campus   \n",
       "5                                          The Porch   \n",
       "\n",
       "                                                 url  \n",
       "1  https://www.airbnb.com/rooms/18520444?location...  \n",
       "2  https://www.airbnb.com/rooms/17481455?location...  \n",
       "3  https://www.airbnb.com/rooms/16926307?location...  \n",
       "4  https://www.airbnb.com/rooms/11839729?location...  \n",
       "5  https://www.airbnb.com/rooms/17325114?location...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first goal on the process is to match the user query with descriptions and tittles of the houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning process:\n",
    "\n",
    "It was noticed that the data base has some \"noise\" within it, for example empty cells (NA) in location coordinates, title and description which is essential for the concept of search engine built and the houses of the respective residences won't produce any match to an entered query.\n",
    "\n",
    "It was also noticed duplicated cells, that sometimes are system error or a house entered in the system twice by the owner, in order to avoid a duplicated outcome for the query the duplicated cells will be dropped out of the data base.\n",
    "\n",
    "Cleaning process:\n",
    "\n",
    "-Delete all rows with (lat,long) equal to NA since if an house is not locatable it is useless for a customer.\n",
    "\n",
    "-Delete rows with both description and title equal to NA since it wouldn't be possible to match the user query.\n",
    "\n",
    "-Retain all others rows, maybe later penalizing records with NA values for other columns(e.g.:bedrooms_count)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T11:40:27.638942Z",
     "start_time": "2018-11-16T11:40:27.633956Z"
    }
   },
   "source": [
    "#### 1. Load the data.csv\n",
    "\n",
    "- read csv file with pandas csv read.\n",
    "\n",
    "#### 2. Call cleandata()\n",
    "\n",
    "- Gets just data which has not null in latitude column;\n",
    "\n",
    "- Drop NA's from description and title;\n",
    "\n",
    "- Drop all duplicated rows;\n",
    "\n",
    "- Returns the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pre-processing the data and creating Tsv files for each house\n",
    "At this point there is a data frame with duplicated removed and empty fields removed.\n",
    "The next step is to go through each row of the data frame process it and write a tsv file with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into text and feature extraction, the first step will be further treating the data in order to obtain better features. We will achieve this by doing some of the basic _*pre-processing*_ steps on our data, for this it was used the library NLTK which has many features for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lower case\n",
    "The first pre-processing step is to transform our descriptions and titles into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘House’ and ‘house’ would be taken as different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Punctuation and Stop Words\n",
    "The next step is to remove punctuation and stopwords, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, it will be used SnowballStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing non-enlish characters\n",
    "When handling the data was noticed non-english texts, for example descriptions entered in chinese language can bring problematics when processing the text. In order to solve this issue was created a function to test if the description an title are indeed using english characters, therefore, the data just proceed for process and creating tsv file if the english test function returns true for the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating TSV files\n",
    "After pre processing the text we join the description and title text and replace then in the original field, proceeding to the tsv file creation. The treated data has 18259 rolls which will result in the same number of tsv files for the analysis of the search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. calls createAllReviews\n",
    "       Calls CreateTSV to all lines of the dataframe com .apply(lambda)\n",
    "        2.CreateTSV (inside a line of the dataframe the following will happen)\n",
    "            if description is empty (.isna) writes \"NaN\" else calls _nltkProcess\n",
    "            the same for title\n",
    "                3. _nltkProcess\n",
    "                    string lower\n",
    "                    4. calls _setupNltk\n",
    "                        Lazy initialization of objects needed to preprocess strings\n",
    "                        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                        stopwords = set(stopwords.words('english'))\n",
    "                        stemmer = SnowballStemmer('english') \n",
    "                    tokenizer\n",
    "                    remove stopwords\n",
    "                    stemm\n",
    "                    join the result and return\n",
    "                5. calls isEnglish() to test over each word int the line if they are in english and not chinese\n",
    "                    try to encode into english, if fails return FALSE for test \"is english\"\n",
    "                    if succeeds return TRUE for the test\n",
    "                if returned TRUE goes on to create the file, if False returns from CreateTSV and don't create the file\n",
    "                open the file to write using with open(self.dir_path+self.review_dir+\"doc_\"+str(x.name)+\".tsv\", 'w') as file:\n",
    "                try to write it\n",
    "                    except (if fails to write) save the index (x.name) to delete the file afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T12:58:16.369236Z",
     "start_time": "2018-11-16T12:58:16.365271Z"
    }
   },
   "source": [
    "### Step 3 -  BuildingEncoding()\n",
    "At this point the tsv files are created and pre-processed. The next step is to create a dictionary for the vocabulary of words present in all the documents, in this dictionary each word corresponds to a single number. This process will help analyse which documents have the respective word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what is the target at this point: {\"car\":1, \"house\":2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step was approached browsing through every file's title and description and adding its words into an empty dictionary. But before adding it is checked if the word isn't yet in the dictionary, when adding the word to the dictionary its key value is a counter of how many words were collected so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.BuildingEncoding\n",
    "        create an empty dictionary to store words\n",
    "        create a counter to store the index of how many single words we have (vocabulary size)\n",
    "        for each file in \"listdir(directory address)\"\n",
    "            goes to the title, strip its words and create a set of it to remove duplicates\n",
    "            do the same for the description\n",
    "            union both sets\n",
    "            for each word in the united set check if it is not dictionary already\n",
    "                if it isnt add it to the dictionary as a key\n",
    "                set the counter as the value of the word in the dictionary\n",
    "                increase the counter +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Reverse Index\n",
    "Creating a dictionary of words as keys and documents as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step can say in for each number (word) which documents have it example \n",
    "\n",
    " \n",
    "{\n",
    "\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "\n",
    "...}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step it is created a new dictionary which the keys are the the vocabulary numbers of the previous dictionary. Then it is checkd for each document if the title and the description has the respective word, if there is the document name is appended as a value to the respective keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create a new dictionary with the keys being the numbers at the size of the vocabulary\n",
    "        this is our target dictionary, we want to say what documents have this word\n",
    "    for each file in \"listdir(directory address)\"\n",
    "        goes to the title, strip its words and create a set of it to remove duplicates\n",
    "        do the same for the description\n",
    "        union both sets\n",
    "        for each word in the set\n",
    "            get which number this word has in the vocabulary dictionary\n",
    "            goes to the new dictionary and append the name of the document to the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T13:06:57.358718Z",
     "start_time": "2018-11-16T13:06:57.353763Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#NOW WE HAVE OUR TARGET DICTIONARY AND WE CAN DO THE INTERSECTION WITH THE QUERY\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#our first go is to get the query and proccess it\n",
    "\n",
    "#enter a string \"a with garden house\"\n",
    "#get the string of the query and tokenize it\n",
    "#filter it eliminating the stopwords\n",
    "#stemm the filtered words and save it into a empty list\n",
    "#the result is ['garden', 'hous']\n",
    "\n",
    "#------------------------------------------------------\n",
    "#now that we have the query words we can go to the dictionary \n",
    "#for every word in the query list\n",
    "    #we append the set of documents that corresponds \n",
    "    #to the word number from the new dictionary into a new list\n",
    "#this new list has all sets of documents which contain the query words\n",
    "#as we want a document which has all the words in it we do an intersection of this documents\n",
    "#WE THEN HAVE ALL THE DOCUMENTS WHICH HAS ALL THE WORDS FROM THE QUERY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
